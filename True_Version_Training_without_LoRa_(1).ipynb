{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "C-WFNPSjFxVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements2.txt"
      ],
      "metadata": {
        "id": "HYJQ_8-DHZ1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNnkgBq7Q3EU"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl==0.7.11"
      ],
      "metadata": {
        "id": "-ko52AT0IGKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnqmq7amRrU8"
      },
      "source": [
        "## Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO8x0J2GtFiJ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant\n",
        "\n",
        "dataset_name = 'Lennard-Heuer/DAIM-LLM'\n",
        "data_files = 'fine_tuning_data (14).jsonl'\n",
        "# data_files = 'Long Exemplary Instruction Tasks.jsonl'  # Or 'traini.jsonl' based on the correct file name\n",
        "dataset_train = load_dataset(dataset_name, data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q93WlzBM27Vh"
      },
      "outputs": [],
      "source": [
        "# for reverence: from datasets import load_dataset\n",
        "\n",
        "# for reverence:  #dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant\n",
        "\n",
        "# for reverence:  dataset_name = 'AlexanderDoria/novel17_test' #french novels\n",
        "# for reverence:  dataset = load_dataset(dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJJHAyx5tp4w"
      },
      "outputs": [],
      "source": [
        "print(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "# Load the configuration\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# Modify the max_position_embeddings to 4096\n",
        "config.max_position_embeddings = 4096\n",
        "\n",
        "# Now, load the model with the modified configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    config=config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Update model configuration to not use cache if needed\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "ABb_vmnS-eeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjOMoSbGSxx9"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNqIYtQcUBSm"
      },
      "source": [
        "Let's also load the tokenizer below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDS2yYmlUAD6"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old stuff"
      ],
      "metadata": {
        "id": "kg8XTgiWhvXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQdvjTYTT1vQ"
      },
      "outputs": [],
      "source": [
        "# from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# lora_alpha = 16\n",
        "# lora_dropout = 0.1\n",
        "# lora_r = 64\n",
        "\n",
        "#peft_config = LoraConfig(\n",
        "#    lora_alpha=lora_alpha,\n",
        "#    lora_dropout=lora_dropout,\n",
        "#    r=lora_r,\n",
        "#    bias=\"none\",\n",
        "#    task_type=\"CAUSAL_LM\"\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_freeze = 28 # you can play with this parameter\n",
        "\n",
        "\n",
        "# freeze layers (disable gradients)\n",
        "for param in model.parameters(): param.requires_grad = False\n",
        "for param in model.lm_head.parameters(): param.requires_grad = True\n",
        "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
      ],
      "metadata": {
        "id": "kuWzmy1uCawf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzsYHLwIZoLm"
      },
      "source": [
        "## Loading the trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTBJVE4PaJwK"
      },
      "source": [
        "Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "used to be 4/4"
      ],
      "metadata": {
        "id": "dGWMybNJi3iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U transformers[torch]"
      ],
      "metadata": {
        "id": "wlQnp3QJUgPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCFTvGW6aspE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "output_dir = \"./results\"\n",
        "per_device_train_batch_size = 1\n",
        "gradient_accumulation_steps = 1\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_steps = 100\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 100\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\"\n",
        "max_seq_length = 512\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3t6b2TkcJwy"
      },
      "source": [
        "Then finally pass everthing to the trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset_train['train']"
      ],
      "metadata": {
        "id": "XlfaGNWrcjGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "ZeAQl6sqTSYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade the trl package to the latest version\n",
        "!pip install --upgrade trl"
      ],
      "metadata": {
        "id": "xid5DbO6JOHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        ")"
      ],
      "metadata": {
        "id": "hgqM7T5gS7ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWplqqDjb3sS"
      },
      "source": [
        "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JApkSrCcL3O"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjvisllacNZM"
      },
      "source": [
        "Now let's train the model! Simply call `trainer.train()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kbS7nRxcMt7"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5c0ppfasK29"
      },
      "source": [
        "During training, the model should converge nicely as follows:\n",
        "\n",
        "![image](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/loss-falcon-7b.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Load the model and tokenizer from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Push to Hugging Face Hub\n",
        "model.push_to_hub(\"Lennard-Heuer/Llama3-FT_V4\")\n",
        "tokenizer.push_to_hub(\"Lennard-Heuer/Llama3-FT_V4\")"
      ],
      "metadata": {
        "id": "Sog1zWoZQKyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Assuming `trainer` is your Trainer instance with the model you've fine-tuned\n",
        "\n",
        "# Save the model; this accounts for parallel/distributed training scenarios\n",
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "model_to_save.save_pretrained(\"outputsV3\")\n",
        "\n",
        "# Assuming you've already logged in to Hugging Face in your environment\n",
        "# If not, you would need to log in using `huggingface_hub.login()`\n",
        "\n",
        "# Push to the Hub. Replace \"your_model_name\" with your desired model name on the Hub\n",
        "# Ensure the model name is unique and descriptive enough\n",
        "trainer.push_to_hub(\"your_model_name\")\n"
      ],
      "metadata": {
        "id": "Lt-agE3cNX4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(\"outputsV4\")\n"
      ],
      "metadata": {
        "id": "hf8KZBqTOPKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "Liu9YpOcPGnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SFTTrainer` also takes care of properly saving only the adapters during training instead of saving the entire model."
      ],
      "metadata": {
        "id": "ZE7r2ujoNce8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"Lennard-Heuer/Llama3-FT-D\")"
      ],
      "metadata": {
        "id": "Ft3V1l9cPlq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2CEgCF14M0m"
      },
      "outputs": [],
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputsV3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwo01QBuV-8h"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig.from_pretrained('outputsV3')\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not in the 1littlecoder skript"
      ],
      "metadata": {
        "id": "llf2CU_i5Y_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHTzcUwDSTtb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Assuming 'your_library' is where LoraConfig is from\n",
        "\n",
        "# Load the base model\n",
        "model = AutoModel.from_pretrained('outputsV3')\n",
        "\n",
        "# Load the adapter configuration\n",
        "# The actual function to load the adapter will depend on the library you are using\n",
        "# Replace 'load_adapter_function' with the actual function name\n",
        "model.load_adapter('outputsV3/adapter_config.json')\n",
        "\n",
        "# Load the LoRA configuration\n",
        "lora_config = LoraConfig.from_pretrained('outputsV3')\n",
        "\n",
        "# Apply the LoRA configuration to the model\n",
        "# This will depend on how the LoRA configuration is applied in your library\n",
        "# Replace 'apply_lora_config_to_model' with the actual function you use to apply the configuration\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmA4G6C64dJ4"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig.from_pretrained('outputsV3')\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWEM89A48NrU"
      },
      "outputs": [],
      "source": [
        "train_dataset['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgt86z-x4diG"
      },
      "outputs": [],
      "source": [
        "text = \"Mention a production planning model\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niyf5_Kc4ugO"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjOInz-Q-54k"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"llama2-qlora-finetunined-XXX-1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lennard-Heuer/llama2-qlora-finetunined-XXX"
      ],
      "metadata": {
        "id": "x1Capfg17vIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "model_name = \"Lennard-Heuer/llama2-qlora-finetunined-XXX\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "XmgREYP275nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "tq9osGkbHeje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "import json\n",
        "import safetensors.torch\n",
        "\n",
        "# Load the base pre-trained LLAMA2 model\n",
        "model = AutoModel.from_pretrained(\"TinyPixel/Llama-2-7B-bf16-sharded\")\n",
        "\n",
        "# Load adapter configuration\n",
        "with open(\"Lennard-Heuer/llama2-qlora-finetunined-XXX/adapter_config.json\", \"r\") as file:\n",
        "    adapter_config = json.load(file)\n",
        "\n",
        "# Assuming the library and model support loading adapters in this way\n",
        "# This part is more conceptual since actual implementation can vary\n",
        "model.load_adapter(\"Lennard-Heuer/llama2-qlora-finetunined-XXX/adapter_model.safetensors\", config=adapter_config)\n",
        "\n",
        "# If safetensors is the format for your adapter_model, ensure you have a method to load it\n",
        "# For example, using safetensors.torch.load if you're working with PyTorch\n",
        "adapter_weights = safetensors.torch.load(\"Lennard-Heuer/llama2-qlora-finetunined-XXX/adapter_model.safetensors\")\n",
        "model.load_state_dict(adapter_weights, strict=False)\n",
        "\n",
        "# Your model is now ready to use with the loaded adapters\n"
      ],
      "metadata": {
        "id": "r5KeUFakMTQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "import json\n",
        "import safetensors.torch\n",
        "\n",
        "# Load the base pre-trained LLAMA2 model\n",
        "model = AutoModel.from_pretrained(\"TinyPixel/Llama-2-7B-bf16-sharded\")\n",
        "\n",
        "# Load adapter configuration\n",
        "# with open(\"Lennard-Heuer/llama2-qlora-finetunined-XXX/adapter_config.json\", \"r\") as file:\n",
        "    # adapter_config = json.load(file)\n"
      ],
      "metadata": {
        "id": "P8QoXy67OEYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import peft\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "D5hHRD5mTMK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JC6ND-GYZ7eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft transformers torch"
      ],
      "metadata": {
        "id": "nHu_DXAjSPeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"Lennard-Heuer/llama2-qlora-finetunined-XXX\""
      ],
      "metadata": {
        "id": "XLqhhAQLUXU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "lY39UGfkUxMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "BQWUJuSAWFcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"“Training models with PEFT and LoRa is cool” ->: \", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "a7chRUaOVHJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config ="
      ],
      "metadata": {
        "id": "LbWhDh7ZUgND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import peft\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "with open(\"Lennard-Heuer/llama2-qlora-finetunined-XXX/adapter_config.json\", \"r\") as file:\n",
        "    adapter_config = json.load(file)\n"
      ],
      "metadata": {
        "id": "47HVfdTeRnr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pGEKojeQR5J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `adapter_config` is loaded from `adapter_config.json`\n",
        "# and `adapter_weights` from `adapter_model.safetensors`\n",
        "\n",
        "# Load adapter configuration\n",
        "with open(\"path_to/adapter_config.json\", \"r\") as file:\n",
        "    adapter_config = json.load(file)\n",
        "\n",
        "# Assuming you have a mechanism to load safetensors into a format compatible with your model\n",
        "# This might involve using a custom loading function or converting safetensors to PyTorch tensors\n",
        "adapter_weights = safetensors.torch.load(\"path_to/adapter_model.safetensors\")\n",
        "\n",
        "# Integrate adapter configuration and weights into the model\n",
        "# This step is highly dependent on the specific mechanisms your adapter and model support\n",
        "# For example, with PEFT and LoRA, you might need to use specific functions to inject the adapter\n",
        "model = get_peft_model(model, adapter_config, peft_config, adapter_weights)\n"
      ],
      "metadata": {
        "id": "OnMZqyu8RmRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Mention a production planning model\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "qI4odIL4Vj8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New Section**"
      ],
      "metadata": {
        "id": "YOtElpZgTNrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements2.txt"
      ],
      "metadata": {
        "id": "GNPztJVuYQmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ],
      "metadata": {
        "id": "sy6fsIi2YNyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft transformers torch"
      ],
      "metadata": {
        "id": "8R64Z0yOYfJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"Lennard-Heuer/llama2-qlora-finetunined-XXX\""
      ],
      "metadata": {
        "id": "8ZxVwNS3YleI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import peft\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "DNEju8MAYbSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "import bitsandbytes"
      ],
      "metadata": {
        "id": "q8dSkAobY_2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "hTkXFm4iPAde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate"
      ],
      "metadata": {
        "id": "-YldMgLRPAKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "jUeZxSwaX0gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"Lennard-Heuer/llama2-qlora-finetunined-XXX\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "8bCStW-mTnIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft transformers torch"
      ],
      "metadata": {
        "id": "Z1XEYOiKZ3dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import peft\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "u8RXqUBOZ-sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"Lennard-Heuer/llama2-qlora-finetunined-XXX\""
      ],
      "metadata": {
        "id": "GprAXFQiZ3dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "ao9qt_vTZ3dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44FskMiGaWR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}