{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSaUx8c76xZZ"
      },
      "source": [
        "# **Chat Interface**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTnkdcJzqFiv"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip -q install huggingface_hub google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9kJ_VEHLw5H"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iuBL-78rhfk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izhC04UupI6S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import langchain\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "from enum import Enum\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMyfKlNUXH0a"
      },
      "outputs": [],
      "source": [
        "!pip install pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGau0Aq3xYvi"
      },
      "outputs": [],
      "source": [
        "import pydantic\n",
        "# Formerly, I capitalized that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkJm1qQwbDW8"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class PersonalDetails(BaseModel):\n",
        "    labor_type: str = Field(\n",
        "        ...,\n",
        "        description=\"If labor costs should be taken into account and what type of labor is currently used (e.g., long-term contracts, long-term contracts, reliance on overtime)?\",\n",
        "    )\n",
        "    capacity_constraining_factors: str = Field(\n",
        "        ...,\n",
        "        description=\"What are currently constraining factors in the plant? (workforce, machine availablitiy?...)\",\n",
        "    )\n",
        "    variable_costs: str = Field(\n",
        "        ...,\n",
        "        description=\"Which variable costs of producing the product are known\",\n",
        "    )\n",
        "    workload_variation: str = Field(\n",
        "        ...,\n",
        "        description=\"Is the workload subject to strong variations?\"\n",
        "    )\n",
        "    product_mix: str = Field(\n",
        "        ...,\n",
        "        description=\"Which products are sold and how is their realtionship to one another?\"\n",
        "    )\n",
        "\n",
        "def check_what_is_empty(user_personal_details):\n",
        "    ask_for = []\n",
        "    # Check if fields are empty\n",
        "    for field, value in user_personal_details.dict().items():\n",
        "        if value in [None, \"\", 0]:  # You can add other 'empty' conditions as per your requirements\n",
        "            ask_for.append(field)\n",
        "    return ask_for\n",
        "\n",
        "def ask_for_info(field_name):\n",
        "    # Create a mapping of field names to human-readable questions\n",
        "    field_to_question = {\n",
        "        'labor_type': \"Can you please provide the type of labor used (e.g., long-term contracts, reliance on overtime)?\",\n",
        "        'capacity_constraining_factors': \"What are currently the constraining factors in the plant (e.g., workforce, machine availability)?\",\n",
        "        'variable_costs': \"Which variable costs of producing the product are known?\",\n",
        "        'workload_variation': \"Is the workload subject to strong variations?\",\n",
        "        'product_mix': \"Which products are sold and how is their relationship to one another?\"\n",
        "    }\n",
        "\n",
        "    # Generate the question for the specific field\n",
        "    question = field_to_question.get(field_name, \"\")\n",
        "    return question\n",
        "\n",
        "def update_user_details(user_details, field_name, response):\n",
        "    # Directly update the field with the response\n",
        "    setattr(user_details, field_name, response)\n",
        "    return user_details\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBoOocOLf-WT"
      },
      "outputs": [],
      "source": [
        "# Initialize user details\n",
        "user_123_personal_details = PersonalDetails(\n",
        "    labor_type=\"\",\n",
        "    capacity_constraining_factors=\"\",\n",
        "    variable_costs=\"\",\n",
        "    workload_variation=\"\",\n",
        "    product_mix=\"\"\n",
        ")\n",
        "\n",
        "user_details = user_123_personal_details\n",
        "\n",
        "while True:\n",
        "    # Check what information is still needed\n",
        "    ask_for = check_what_is_empty(user_details)\n",
        "\n",
        "    # If all information is gathered, exit the loop\n",
        "    if not ask_for:\n",
        "        print(\"All information gathered. Moving to the next phase.\")\n",
        "        break\n",
        "\n",
        "    # Get the next field that needs to be filled\n",
        "    next_field = ask_for[0]\n",
        "\n",
        "    # Generate a conversational question based on the missing information\n",
        "    ai_response = ask_for_info(next_field)\n",
        "    print(ai_response)\n",
        "\n",
        "    # Get user's response\n",
        "    user_response = input(\"> \")\n",
        "\n",
        "    # Process the response and update user details\n",
        "    user_details = update_user_details(user_details, next_field, user_response)\n",
        "\n",
        "print(\"Final user details:\", user_details.dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHRTLQY4wf1v"
      },
      "outputs": [],
      "source": [
        "user_details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGnyHpYoPebX"
      },
      "source": [
        "# **Decision Making with Factory Physics - RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aefdyBNfrfcm"
      },
      "outputs": [],
      "source": [
        "!pip install pulp\n",
        "!pip install python-dotenv\n",
        "!pip install -U chromadb\n",
        "!pip install -U tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l4vYqK9e2Ds"
      },
      "source": [
        "New try with the GPT4 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjqRGWSpfDmm"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQekT9QsfK0_"
      },
      "outputs": [],
      "source": [
        "#loader = TextLoader(file_path=\"../data/PaulGrahamEssays/vb.txt\")\n",
        "\n",
        "## Other options for loaders\n",
        "# loader = PyPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42sjU-DSh8bL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define the path to your file\n",
        "file_path = 'train (2).jsonl'\n",
        "\n",
        "# Define a simple Document class to hold content and metadata\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata else {}\n",
        "\n",
        "# Read the JSONL file and convert chunks to Document objects\n",
        "documents = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        chunk = data['chunk']\n",
        "        documents.append(Document(page_content=chunk))\n",
        "\n",
        "# Check the number of documents loaded\n",
        "print(f'Loaded {len(documents)} documents.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o0zgIEDkqLX"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM0Tta93liMW"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "import tiktoken\n",
        "# Load data into Chroma vector store\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL51bjwLnN3I"
      },
      "outputs": [],
      "source": [
        "# Define your query\n",
        "query = f\"Which basic model and which extension(s) of the basic model should I use. You must return one or more of the 8 models, while combinations are also possible: Which production planning model from the book factory physics would be the most appropriate and why? The possible answers are: Aggregate Planning (AP), Single Product, I. Basic model; Aggregate Planning (AP), Product Mix Planning II. Basic model; Aggregate Planning (AP), Product Mix Planning III. Capacitated ressources; Aggregate Planning (AP), Product Mix Planning IV. Utilization Matching; Aggregate Planning (AP), Product Mix Planning V. Backorders; Aggregate Planning (AP), Product Mix Planning VI. Overtime; Aggregate Planning (AP), Product Mix Planning VII. Yield loss; Workforce Planning (WP), VIII. Basic model. The company's envorinment is as follows: {user_details}\"\n",
        "\n",
        "# Perform similarity search\n",
        "docs = vectorstore.similarity_search(query, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIn0XBiVnbAJ"
      },
      "outputs": [],
      "source": [
        "# Initialize the LLM and QA chain using GPT-4\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OVy3yAungQy"
      },
      "outputs": [],
      "source": [
        "# Run the query through the chain\n",
        "answer = chain.run(input_documents=docs, question=query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfuU5bFHzBad"
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
